{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Word Counts\n",
    "\n",
    "Perform an advanced frequency analysis on words in a given text source.\n",
    "1. Convert a text file into a string.\n",
    "2. Split a string into words, excluding punctuation marks.\n",
    "3. Remove stop words from the string.\n",
    "4. Lemmatize the words in the string so that all words are stem words.\n",
    "5. Count the frequency of each stem word and store the results in a dictionary.\n",
    "6. Convert the dictionary to a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Convert a Text File to a String\n",
    "Create a function that takes as input the path to a text file and returns the contents of the file as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"data/text.txt\"):\n",
    "    print(\"File already exist.\")\n",
    "else:\n",
    "    f = open(\"data/text.txt\", \"a\")\n",
    "    f.write(\"Hello World!\")\n",
    "    f.close()\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, 'r')\n",
    "    r = f.read()\n",
    "    return r\n",
    "text = read_text_file(\"data/text.txt\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the String into Words\n",
    "Create a function that takes as input a string and returns a list of strings representing the words in the text file.\n",
    "\n",
    "The function should divide the string into words based on any type of punctuation.\n",
    "\n",
    "The function should convert all words into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "def split_text(text):\n",
    "    \n",
    "    if os.path.exists(\"data/file.txt\") == False:\n",
    "        f = open(\"data/file.txt\", \"a\")\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "    f = open(\"data/file.txt\", \"r\")\n",
    "    content = f.read()\n",
    "    lower_case = content.lower()\n",
    "    \n",
    "    # content_list = lower_case.split(\" \")\n",
    "    \n",
    "    content_list = re.split(r\"[-;,.\\s]\\s*\", lower_case)\n",
    "    \n",
    "    f.close()\n",
    "    return (content_list)\n",
    "    \n",
    "text = 'Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise, but admirably balanced mind. He was, I take it, the most perfect reasoning and observing machine that the world has seen; but, as a lover, he would have placed himself in a false position. He never spoke of the softer passions, save with a gibe and a sneer. They were admirable things for the observer—excellent for drawing the veil from men’s motives and actions. But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his. And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.'\n",
    "words = split_text(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exclude Stop Words\n",
    "When searching or indexing text content (such as web pages or large documents), we typically want to exclude frequently-used words like \"the,\" \"a,\" or \"and\" so that the search or analysis includes only the words that are more likely to produce meaningful results. We use the term \"stop words\" to reference this collection of words.\n",
    "\n",
    "Because this is a common task when working with text, Python has an nltk module that includes stop words for a variety of languages. We can use this module to remove stop words from text we want to search or analyze.\n",
    "\n",
    "Create a function that takes as input a list of words and removes all stop words. The basic steps of importing the stopwords module are provided for you, but you may find it useful to do more research on stop words before completing this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    " \n",
    "nltk.download('stopwords')\n",
    " \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(words,stop_words):\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "words = ['sherlock', 'holmes', 'she', 'is', 'always', 'the', 'woman', 'i', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', 'in', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', 'it', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'irene', 'adler', 'all', 'emotions', 'and', 'that', 'one', 'particularly', 'were', 'abhorrent', 'to', 'his', 'cold', 'precise', 'but', 'admirably', 'balanced', 'mind', 'he', 'was', 'i', 'take', 'it', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', 'he', 'never', 'spoke', 'of', 'the', 'softer', 'passions', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', 'they', 'were', 'admirable', 'things', 'for', 'the', 'observer—excellent', 'for', 'drawing', 'the', 'veil', 'from', 'men’s', 'motives', 'and', 'actions', 'but', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', 'grit', 'in', 'a', 'sensitive', 'instrument', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high', 'power', 'lenses', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', 'and', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', 'and', 'that', 'woman', 'was', 'the', 'late', 'irene', 'adler', 'of', 'dubious', 'and', 'questionable', 'memory', '']\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "print(words_clean)\n",
    "print(len(words_clean))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Lemmatize the Words\n",
    "We can also use the nltk module to lemmatize words in a text file. The term lemmatize refers to the process of identifying words that are inflected versions of the same stem word, so that only the stem word is included in the analysis.\n",
    "\n",
    "For example, each of the following phrases includes an inflected form of the stem word \"walk\":\n",
    "\n",
    "I walked to the coffee shop last night.\n",
    "Helen regularly walks her dog in the evening.\n",
    "They saw the boys walking toward the house.\n",
    "A strict textual analysis would count each of these as a separate word, but they are all actually different forms of the same stem word, \"walk.\" Lemmatizing the words reduces the number of words that a process must analyze, making the process more efficient and the results more meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "word = \"priorities\"\n",
    "word_lemmatized =  lemmatizer.lemmatize(word)\n",
    "print(word) #original word\n",
    "print(word_lemmatized) #lemmatized word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a function to lemmatize each word in a list of words produced in the previous step of this activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to complete the activity\n",
    "def lemmatize_words(words_clean):\n",
    "    lem_words = []\n",
    "    for word in words_clean:\n",
    "        word_lemmatized = lemmatizer.lemmatize(word)\n",
    "        lem_words.append(word_lemmatized)\n",
    "    return lem_words\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "print(words_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Count the Words\n",
    "Create a function that takes as input a list of lemmatized words and returns a dictionary that has the frequency of occurrence of each lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lemmatized = ['sherlock', 'holmes', 'always', 'woman', 'seldom', 'heard', 'mention', 'name', 'eye', 'eclipse', 'predominates', 'whole', 'sex', 'felt', 'emotion', 'akin', 'love', 'irene', 'adler', 'emotion', 'one', 'particularly', 'abhorrent', 'cold', 'precise', 'admirably', 'balanced', 'mind', 'take', 'perfect', 'reasoning', 'observing', 'machine', 'world', 'seen', 'lover', 'would', 'placed', 'false', 'position', 'never', 'spoke', 'softer', 'passion', 'save', 'gibe', 'sneer', 'admirable', 'thing', 'observer—excellent', 'drawing', 'veil', 'men’s', 'motif', 'action', 'trained', 'reasoner', 'admit', 'intrusion', 'delicate', 'finely', 'adjusted', 'temperament', 'introduce', 'distracting', 'factor', 'might', 'throw', 'doubt', 'upon', 'mental', 'result', 'grit', 'sensitive', 'instrument', 'crack', 'one', 'high', 'power', 'lens', 'would', 'disturbing', 'strong', 'emotion', 'nature', 'yet', 'one', 'woman', 'woman', 'late', 'irene', 'adler', 'dubious', 'questionable', 'memory']\n",
    "def compute_frequency_words(words_lemmatized):\n",
    "    word_freq = dict()\n",
    "    # Create a dictionary of words as key and count as value, starting with zero first\n",
    "    for i in words_lemmatized:\n",
    "        word_freq[i] = 0\n",
    "\n",
    "    # Loop through the list of words and if the word appears as a key in the dictionary append 1 to the value\n",
    "    for i in words_lemmatized:\n",
    "        if i in word_freq:\n",
    "            word_freq[i] += 1\n",
    "    # print(\"Word Frequency: \", word_freq)\n",
    "    return word_freq\n",
    "\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "print(type(words_frequency)) #should print dict\n",
    "print(words_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Export the Results to JSON\n",
    "Create a function that takes as input a dictionary where the key is a word and the value is the frequency of occurrence of that word in an input text.\n",
    "\n",
    "The function should store the dictionary in a JSON file named words_frequency.json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dict = {'sherlock': 1, 'holmes': 1, 'always': 1, 'woman': 3, 'seldom': 1, 'heard': 1, 'mention': 1, 'name': 1, 'eye': 1, 'eclipse': 1, 'predominates': 1, 'whole': 1, 'sex': 1, 'felt': 1, 'emotion': 3, 'akin': 1, 'love': 1, 'irene': 2, 'adler': 2, 'one': 3, 'particularly': 1, 'abhorrent': 1, 'cold': 1, 'precise': 1, 'admirably': 1, 'balanced': 1, 'mind': 1, 'take': 1, 'perfect': 1, 'reasoning': 1, 'observing': 1, 'machine': 1, 'world': 1, 'seen': 1, 'lover': 1, 'would': 2, 'placed': 1, 'false': 1, 'position': 1, 'never': 1, 'spoke': 1, 'softer': 1, 'passion': 1, 'save': 1, 'gibe': 1, 'sneer': 1, 'admirable': 1, 'thing': 1, 'observer—excellent': 1, 'drawing': 1, 'veil': 1, 'men’s': 1, 'motif': 1, 'action': 1, 'trained': 1, 'reasoner': 1, 'admit': 1, 'intrusion': 1, 'delicate': 1, 'finely': 1, 'adjusted': 1, 'temperament': 1, 'introduce': 1, 'distracting': 1, 'factor': 1, 'might': 1, 'throw': 1, 'doubt': 1, 'upon': 1, 'mental': 1, 'result': 1, 'grit': 1, 'sensitive': 1, 'instrument': 1, 'crack': 1, 'high': 1, 'power': 1, 'lens': 1, 'disturbing': 1, 'strong': 1, 'nature': 1, 'yet': 1, 'late': 1, 'dubious': 1, 'questionable': 1, 'memory': 1}\n",
    "\n",
    "# convert a dictionary into a string object that we can display. \n",
    "json_data = json.dumps(json_dict, indent = 4) \n",
    "print(json_dict) # dict\n",
    "print(type(json_dict))\n",
    " \n",
    "print(json_data) # string\n",
    "print(type(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "json_dict = {'sherlock': 1, 'holmes': 1, 'always': 1, 'woman': 3, 'seldom': 1, 'heard': 1, 'mention': 1, 'name': 1, 'eye': 1, 'eclipse': 1, 'predominates': 1, 'whole': 1, 'sex': 1, 'felt': 1, 'emotion': 3, 'akin': 1, 'love': 1, 'irene': 2, 'adler': 2, 'one': 3, 'particularly': 1, 'abhorrent': 1, 'cold': 1, 'precise': 1, 'admirably': 1, 'balanced': 1, 'mind': 1, 'take': 1, 'perfect': 1, 'reasoning': 1, 'observing': 1, 'machine': 1, 'world': 1, 'seen': 1, 'lover': 1, 'would': 2, 'placed': 1, 'false': 1, 'position': 1, 'never': 1, 'spoke': 1, 'softer': 1, 'passion': 1, 'save': 1, 'gibe': 1, 'sneer': 1, 'admirable': 1, 'thing': 1, 'observer—excellent': 1, 'drawing': 1, 'veil': 1, 'men’s': 1, 'motif': 1, 'action': 1, 'trained': 1, 'reasoner': 1, 'admit': 1, 'intrusion': 1, 'delicate': 1, 'finely': 1, 'adjusted': 1, 'temperament': 1, 'introduce': 1, 'distracting': 1, 'factor': 1, 'might': 1, 'throw': 1, 'doubt': 1, 'upon': 1, 'mental': 1, 'result': 1, 'grit': 1, 'sensitive': 1, 'instrument': 1, 'crack': 1, 'high': 1, 'power': 1, 'lens': 1, 'disturbing': 1, 'strong': 1, 'nature': 1, 'yet': 1, 'late': 1, 'dubious': 1, 'questionable': 1, 'memory': 1}\n",
    "with open('data/words_frequency.json', 'w') as outfile:\n",
    "    json.dump(json_dict, outfile)\n",
    "f = open('data/words_frequency.json', 'r')\n",
    "pprint(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Combine All Steps in a Single Program\n",
    "Using the skeleton below, combine all of the previous steps into a single script that will perform the following steps:\n",
    "\n",
    "1. Convert a text file to a string.\n",
    "2. Split the string into words, excluding punctuation marks.\n",
    "3. Remove stop words from the list of strings.\n",
    "4. Lemmatize the words in the list so that all words are stem words.\n",
    "5. Count the frequency of each stem word and store the results in a dictionary.\n",
    "6. Convert the dictionary to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import json\n",
    "\n",
    "\n",
    "def string_freq(file_path):\n",
    "    # Read text file to a string\n",
    "    f = open(file_path, 'r')\n",
    "    content = f.read()\n",
    "    # Split string into words excluding punctuation marks:\n",
    "    lower_case = content.lower()\n",
    "    content_list = re.split(r\"[-;,.\\s]\\s*\", lower_case)\n",
    "    \n",
    "    # Remove stop words:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    for w in content_list:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    # filtered_sentence = filtered_sentence.remove(\"\\''\")\n",
    "    \n",
    "    # Lemmatize the words:\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lem_words = []\n",
    "    for word in filtered_sentence:\n",
    "        word_lemmatized = lemmatizer.lemmatize(word)\n",
    "        lem_words.append(word_lemmatized)\n",
    "    \n",
    "    # Count the frequency of each stem word and store them in a dictionary\n",
    "    word_freq = dict()\n",
    "    # Create a dictionary of words as key and count as value, starting with zero first\n",
    "    for i in words_lemmatized:\n",
    "        word_freq[i] = 0\n",
    "\n",
    "    # Loop through the list of words and if the word appears as a key in the dictionary append 1 to the value:\n",
    "    for i in words_lemmatized:\n",
    "        if i in word_freq:\n",
    "            word_freq[i] += 1\n",
    "\n",
    "    # Convert the dictionary into a JSON file:\n",
    "    with open('data/words_frequency.json', 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "        f = open('data/words_frequency.json', 'r')\n",
    "        p = pprint(f.read())\n",
    "    return p\n",
    "text = \"data/file.txt\"\n",
    "print(string_freq(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise, but admirably balanced mind. He was, I take it, the most perfect reasoning and observing machine that the world has seen; but, as a lover, he would have placed himself in a false position. He never spoke of the softer passions, save with a gibe and a sneer. They were admirable things for the observer—excellent for drawing the veil from men’s motives and actions. But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his. And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sherlock\": 1, \"holmes\": 1, \"always\": 1, \"woman\": 3, \"seldom\": 1, \"heard\": 1, \"mention\": 1, \"name\": 1, \"eye\": 1, \"eclipse\": 1, \"predominates\": 1, \"whole\": 1, \"sex\": 1, \"felt\": 1, \"emotion\": 3, \"akin\": 1, \"love\": 1, \"irene\": 2, \"adler\": 2, \"one\": 3, \"particularly\": 1, \"abhorrent\": 1, \"cold\": 1, \"precise\": 1, \"admirably\": 1, \"balanced\": 1, \"mind\": 1, \"take\": 1, \"perfect\": 1, \"reasoning\": 1, \"observing\": 1, \"machine\": 1, \"world\": 1, \"seen\": 1, \"lover\": 1, \"would\": 2, \"placed\": 1, \"false\": 1, \"position\": 1, \"never\": 1, \"spoke\": 1, \"softer\": 1, \"passion\": 1, \"save\": 1, \"gibe\": 1, \"sneer\": 1, \"admirable\": 1, \"thing\": 1, \"observer\\u2014excellent\": 1, \"drawing\": 1, \"veil\": 1, \"men\\u2019s\": 1, \"motif\": 1, \"action\": 1, \"trained\": 1, \"reasoner\": 1, \"admit\": 1, \"intrusion\": 1, \"delicate\": 1, \"finely\": 1, \"adjusted\": 1, \"temperament\": 1, \"introduce\": 1, \"distracting\": 1, \"factor\": 1, \"might\": 1, \"throw\": 1, \"doubt\": 1, \"upon\": 1, \"mental\": 1, \"result\": 1, \"grit\": 1, \"sensitive\": 1, \"instrument\": 1, \"crack\": 1, \"high\": 1, \"power\": 1, \"lens\": 1, \"disturbing\": 1, \"strong\": 1, \"nature\": 1, \"yet\": 1, \"late\": 1, \"dubious\": 1, \"questionable\": 1, \"memory\": 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import json\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, 'r')\n",
    "    r = f.read()\n",
    "    f.close()\n",
    "    return r\n",
    "def split_text(text):\n",
    "    lower_case = text.lower()\n",
    "    content_list = re.split(r\"[-;,.\\s]\\s*\", lower_case)\n",
    "    try:\n",
    "        while True:\n",
    "            content_list.remove('')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return content_list\n",
    "def remove_stop_words(words,stop_words):\n",
    "    filtered_sentence = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "def lemmatize_words(words_clean):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lem_words = []\n",
    "    for word in words_clean:\n",
    "        word_lemmatized = lemmatizer.lemmatize(word)\n",
    "        lem_words.append(word_lemmatized)\n",
    "    return lem_words\n",
    "def compute_frequency_words(words_lemmatized):\n",
    "    word_freq = dict()\n",
    "    # Create a dictionary of words as key and count as value, starting with zero first\n",
    "    for i in words_lemmatized:\n",
    "        word_freq[i] = 0\n",
    "\n",
    "    # Loop through the list of words and if the word appears as a key in the dictionary append 1 to the value:\n",
    "    for i in words_lemmatized:\n",
    "        if i in word_freq:\n",
    "            word_freq[i] += 1\n",
    "    return word_freq\n",
    "def save_words_frequency(words_frequency,file_path=\"data/words_frequency.json\"):\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        json.dump(words_frequency, outfile)\n",
    "        \n",
    "    f = open(file_path, 'r')\n",
    "    print(f.read())\n",
    "    f.close()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = read_text_file(\"data/text.txt\")\n",
    "words = split_text(text)\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "save_words_frequency(words_frequency,file_path=\"data/words_frequency.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
