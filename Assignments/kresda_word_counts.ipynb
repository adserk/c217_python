{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Word Counts\n",
    "\n",
    "Perform an advanced frequency analysis on words in a given text source.\n",
    "1. Convert a text file into a string.\n",
    "2. Split a string into words, excluding punctuation marks.\n",
    "3. Remove stop words from the string.\n",
    "4. Lemmatize the words in the string so that all words are stem words.\n",
    "5. Count the frequency of each stem word and store the results in a dictionary.\n",
    "6. Convert the dictionary to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words' Frequency Sorted by Most-Least Frequent:  [('little', 2), ('holmes', 2), ('week', 2), ('clearing', 2), ('time', 2), ('seen', 1), ('lately', 1), ('marriage', 1), ('drifted', 1), ('u', 1), ('away', 1), ('complete', 1), ('happiness', 1), ('home', 1), ('centred', 1), ('interest', 1), ('rise', 1), ('around', 1), ('man', 1), ('first', 1), ('find', 1), ('master', 1), ('establishment', 1), ('sufficient', 1), ('absorb', 1), ('attention', 1), ('loathed', 1), ('every', 1), ('form', 1), ('society', 1), ('whole', 1), ('bohemian', 1), ('soul', 1), ('remained', 1), ('lodging', 1), ('baker', 1), ('street', 1), ('buried', 1), ('among', 1), ('old', 1), ('book', 1), ('alternating', 1), ('cocaine', 1), ('ambition', 1), ('drowsiness', 1), ('drug', 1), ('fierce', 1), ('energy', 1), ('keen', 1), ('nature', 1), ('still', 1), ('ever', 1), ('deeply', 1), ('attracted', 1), ('study', 1), ('crime', 1), ('occupied', 1), ('immense', 1), ('faculty', 1), ('extraordinary', 1), ('power', 1), ('observation', 1), ('following', 1), ('clue', 1), ('mystery', 1), ('abandoned', 1), ('hopeless', 1), ('official', 1), ('police', 1), ('heard', 1), ('vague', 1), ('account', 1), ('doings:', 1), ('summons', 1), ('odessa', 1), ('case', 1), ('trepoff', 1), ('murder', 1), ('singular', 1), ('tragedy', 1), ('atkinson', 1), ('brother', 1), ('trincomalee', 1), ('finally', 1), ('mission', 1), ('accomplished', 1), ('delicately', 1), ('successfully', 1), ('reigning', 1), ('family', 1), ('holland', 1), ('beyond', 1), ('sign', 1), ('activity', 1), ('however', 1), ('merely', 1), ('shared', 1), ('reader', 1), ('daily', 1), ('press', 1), ('knew', 1), ('former', 1), ('friend', 1), ('companion', 1)]\n",
      "\n",
      "\n",
      "Words Sorted Alphabetically:  ['away', 'around', 'absorb', 'attention', 'among', 'alternating', 'ambition', 'attracted', 'abandoned', 'account', 'atkinson', 'accomplished', 'activity', 'bohemian', 'baker', 'buried', 'book', 'brother', 'beyond', 'complete', 'centred', 'cocaine', 'crime', 'clue', 'clearing', 'case', 'companion', 'drifted', 'drowsiness', 'drug', 'deeply', 'doings:', 'delicately', 'daily', 'establishment', 'every', 'energy', 'ever', 'extraordinary', 'first', 'find', 'form', 'fierce', 'faculty', 'following', 'finally', 'family', 'former', 'friend', 'holmes', 'happiness', 'home', 'hopeless', 'heard', 'holland', 'however', 'interest', 'immense', 'keen', 'knew', 'little', 'lately', 'loathed', 'lodging', 'marriage', 'man', 'master', 'mystery', 'murder', 'mission', 'merely', 'nature', 'old', 'occupied', 'observation', 'official', 'odessa', 'power', 'police', 'press', 'rise', 'remained', 'reigning', 'reader', 'seen', 'sufficient', 'society', 'soul', 'street', 'still', 'study', 'summons', 'singular', 'successfully', 'sign', 'shared', 'time', 'trepoff', 'tragedy', 'trincomalee', 'u', 'vague', 'whole', 'week']\n"
     ]
    }
   ],
   "source": [
    "# Kresda Rattanasudsai\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import json\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, 'r')\n",
    "    r = f.read()\n",
    "    f.close()\n",
    "    return r\n",
    "\n",
    "def split_text(text):\n",
    "    lower_case = text.lower()\n",
    "    content_list = re.split(r\"[-;,.\\s]\\s*\", lower_case)\n",
    "    try:\n",
    "        while True:\n",
    "            content_list.remove('')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return content_list\n",
    "\n",
    "def remove_stop_words(words,stop_words):\n",
    "    filtered_sentence = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def lemmatize_words(words_clean):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lem_words = []\n",
    "    for word in words_clean:\n",
    "        word_lemmatized = lemmatizer.lemmatize(word)\n",
    "        lem_words.append(word_lemmatized)\n",
    "    return lem_words\n",
    "\n",
    "def compute_frequency_words(words_lemmatized):\n",
    "    word_freq = dict()\n",
    "    # Create a dictionary of words as key and count as value, starting with zero first\n",
    "    for i in words_lemmatized:\n",
    "        word_freq[i] = 0\n",
    "\n",
    "    # Loop through the list of words and if the word appears as a key in the dictionary append 1 to the value:\n",
    "    for i in words_lemmatized:\n",
    "        if i in word_freq:\n",
    "            word_freq[i] += 1\n",
    "    return word_freq\n",
    "    \n",
    "def save_words_frequency(words_frequency,file_path=\"data/words_frequency.json\"):\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        json.dump(words_frequency, outfile)\n",
    "        \n",
    "    f = open(file_path, 'r')\n",
    "    # print(f.read())\n",
    "    f.close()\n",
    "\n",
    "# Calling the functions to read from text.txt file and find the words' frequency\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = read_text_file(\"data/text.txt\")\n",
    "words = split_text(text)\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "save_words_frequency(words_frequency,file_path=\"data/words_frequency.json\")\n",
    "\n",
    "#sort the words' frequency (value) by largest to smallest as a list\n",
    "sorted_frequency = sorted(words_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Words' Frequency Sorted by Most-Least Frequent: \", sorted_frequency)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Sort the words (keys) alphabetically \n",
    "sorted_words = sorted(words_frequency.keys(), key=lambda x: x[0].lower())\n",
    "print(\"Words Sorted Alphabetically: \", sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8822641e88d7c74114f38a155dc8686f9f41cc7c790ba54cfc07cc82201c3e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
